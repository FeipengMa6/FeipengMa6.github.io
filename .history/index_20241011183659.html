<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Feipeng Ma</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
  </head>

  <body>
    <table style="width:100%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Feipeng Ma (马飞鹏)
                </p>
                <p>
                I am a 4th-year Ph.D. student of <a href="https://en.ustc.edu.cn/">University of Science and Technology of China</a>, supervised by <a href="https://scholar.google.com/citations?user=VRG3dw4AAAAJ&hl=zh-CN">Xiaoyan Sun</a> and <a href="https://scholar.google.com/citations?user=LatWlFAAAAAJ&hl=zh-CN&oi=ao">Yueyi Zhang</a>.
                I am a fourth-year Ph.D. student at the <a href="https://en.ustc.edu.cn/">University of Science and Technology of China</a>, supervised by <a href="https://scholar.google.com/citations?user=VRG3dw4AAAAJ&hl=zh-CN">Xiaoyan Sun</a> and <a href="https://scholar.google.com/citations?user=LatWlFAAAAAJ&hl=zh-CN&oi=ao">Yueyi Zhang</a>. Currently, I am a research intern at WeChat, Tencent Inc., with <a href="https://scholar.google.com/citations?user=dHBNmSkAAAAJ&hl=zh-CN&oi=ao">Yizhou Zhou</a> as my mentor. 
                <br><br>
                My research interests include computer vision, multimodal learning, and large multimodal models.
                </p>
                <p style="text-align:left">
                  <a href="https://scholar.google.com/citations?user=ZDxabCwAAAAJ&hl=zh-CN&oi=ao">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="mailto:mafp@mail.ustc.edu.cn">Email</a> &nbsp;/&nbsp;
                  <a href="https://github.com/FeipengMa6/">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/feipengma.jpeg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/dino.png" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Publications</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    
    <!-- EE-MLLM -->
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one;">
          <img src='images/EE-MLLM.png' width=100%; height="auto">
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2408.11795">
          <span class="papertitle">EE-MLLM: A Data-Efficient and Compute-Efficient Multimodal Large Language Model</span>
        </a>
        <br>
        <strong>Feipeng Ma</strong>,
        Yizhou Zhou, Hebei Li, Zilong He, Siying Wu,
        Fengyun Rao, Yueyi Zhang, Xiaoyan Sun
        <br>
        <em>USTC</em> / <em>WeChat</em>
        <br>
        <em>arXiv</em>, 2024
        <br>
        <a href="https://arxiv.org/pdf/2408.11795">[arXiv]</a>
        <p></p>
      </td>
    </tr>

    <!-- MM-GEM -->
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one;">
          <img src='images/mm-gem.png' width=100%; height="auto">
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2405.19333">
          <span class="papertitle">Multi-Modal Generative Embedding Model</span>
        </a>
        <br>
        <strong>Feipeng Ma</strong>,
        Hongwei Xue, Guangting Wang, Yizhou Zhou, Fengyun Rao, Shilin Yan, Yueyi Zhang, Siying Wu, Mike Zheng Shou, Xiaoyan Sun
        <br>
        <em>USTC</em> / <em>WeChat</em> / <em>NUS</em> / <em>FDU</em>
        <br>
        <em>arXiv</em>, 2024
        <br>
        <a href="https://arxiv.org/pdf/2405.19333">[arXiv]</a>
        <p></p>
      </td>
    </tr>

    <!-- VLoRA -->
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one;">
          <img src='images/vlora.png' width=100%; height="auto">
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2405.20339">
          <span class="papertitle">Visual Perception by Large Language Model's Weights</span>
        </a>
        <br>
        <strong>Feipeng Ma</strong>,
        Hongwei Xue, Yizhou Zhou, Guangting Wang, Fengyun Rao, Shilin Yan, Yueyi Zhang, Siying Wu, Mike Zheng Shou, Xiaoyan Sun
        <br>
        <em>USTC</em> / <em>WeChat</em> / <em>NUS</em> / <em>FDU</em>
        <br>
        <em>NeurIPS</em>, 2024
        <br>
        <a href="https://arxiv.org/pdf/2405.20339">[arXiv]</a>
        /
        <a href="https://github.com/FeipengMa6/VLoRA">[code]</a>
        /
        <a href="https://feipengma6.github.io/vlora">[project]</a>
        <p></p>
      </td>
    </tr>

    <!-- Task Navigator -->
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one;">
          <img src='images/task-navigator.png' width=100%; height="auto">
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://openaccess.thecvf.com/content/CVPR2024W/MAR/papers/Ma_Task_Navigator_Decomposing_Complex_Tasks_for_Multimodal_Large_Language_Models_CVPRW_2024_paper.pdf">
          <span class="papertitle">Task Navigator: Decomposing Complex Tasks for Multimodal Large Language Models</span>
        </a>
        <br>
        <strong>Feipeng Ma</strong>,
        Yizhou Zhou, Yueyi Zhang, Siying Wu, Zheyu Zhang, Zilong He, Fengyun Rao, Xiaoyan Sun
        <br>
        <em>USTC</em> / <em>WeChat</em>
        <br>
        <em>CVPR Workshop</em>, 2024
        <br>
        <a href="https://openaccess.thecvf.com/content/CVPR2024W/MAR/papers/Ma_Task_Navigator_Decomposing_Complex_Tasks_for_Multimodal_Large_Language_Models_CVPRW_2024_paper.pdf">[paper]</a>
        <p></p>
      </td>
    </tr>

    <!-- ICSD -->
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one;">
          <img src='images/ICSD.png' width=100%; height="auto">
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2305.18072.pdf">
          <span class="papertitle">Image Captioning with Multi-Context Synthetic Data</span>
        </a>
        <br>
        <strong>Feipeng Ma</strong>,
        Yizhou Zhou, Fengyun Rao, Yueyi Zhang, Xiaoyan Sun
        <br>
        <em>USTC</em> / <em>WeChat</em>
        <br>
        <em>AAAI</em>, 2024
        <br>
        <a href="https://arxiv.org/pdf/2305.18072.pdf">[arXiv]</a>
        
        <p></p>
        <!-- <p>
          We propose a pipeline called ICSD that generates multi-context training data by combining LLMs and diffusion
          models for image captioning.
        </p> -->
      </td>
    </tr>
	
    <!-- PriSA -->
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one;">
          <img src='images/PriSA.png' width="100%"; height="auto">
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://ieeexplore.ieee.org/abstract/document/10219768">
          <span class="papertitle">Multimodal Sentiment Analysis with Preferential Fusion and Distance-aware Contrastive Learning</span>
        </a>
        <br>
        <strong>Feipeng Ma</strong>,
        Yueyi Zhang, Xiaoyan Sun
        <br>
        <em>USTC</em>
        <br>
        <em>ICME</em>, 2023 &nbsp <font color="red"><strong>(Oral)</strong></font>
        <br>
        <a href="https://github.com/FeipengMa6/PriSA">[code]</a>/
        <br>
        <p></p>
        <p>
          <!-- We address the issue that an excessive reliance on the text modality may lead to the learning of false correlations between textual tokens and sentiment labels, resulting in errors in sentiment analysis. -->
        </p>
      </td>
    </tr>  



    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
      <td style="padding:20px;width:100%;vertical-align:middle">
        <h2>Competitions</h2>
      </td>
    </tr>
  </tbody></table>
  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
            <img src='images/VSC_Descriptor.png' width="100%"; height="auto"></div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2006.04902">
          <span class="papertitle">Meta AI Video Similarity Challenge: Descriptor Track</span>
        </a>


        <br>
        <em>CVPR</em>, 2023 &nbsp <font color="red"><strong>Rank 1</strong></font>
        <br>
        <a href="https://arxiv.org/pdf/2305.12361.pdf">[Technical Report]</a>
        /
        <a href="https://github.com/FeipengMa6/VSC22-Submission">[code]</a>
        <!-- / -->
        <!-- <a href="https://www.youtube.com/watch?v=zhO8iUBpnCc">video</a> -->
        <br>
        <p></p>

      </td>
    </tr>  
    

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
            <img src='images/VSC_Matching.png' width="100%"; height="auto"></div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2006.04902">
          <span class="papertitle">Meta AI Video Similarity Challenge: Matching Track</span>
        </a>


        <br>
        <em>CVPR</em>, 2023 &nbsp <font color="red"><strong>Rank 1</strong></font>
        <br>
        <a href="https://arxiv.org/pdf/2305.15679.pdf">[Technical Report]</a>
        /
        <a href="https://github.com/FeipengMa6/VSC22-Submission">[code]</a>
        <!-- / -->
        <!-- <a href="https://www.youtube.com/watch?v=zhO8iUBpnCc">video</a> -->
        <br>
        <p></p>

      </td>
    </tr>  

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>
      <tr>
      <td style="padding:20px;width:100%;vertical-align:middle">
        <h2>Experience</h2>
      </td>
    </tr>
  </tbody></table>
  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  
  <tr>
    <td style="padding:10px 20px;vertical-align:middle">
      <strong>WeChat, Tencent Inc.</strong>, Beijing, China
      <br>
      Research Intern, Jan. 2023 - Present
    </td>
  </tr>

  <tr></tr>
    <td style="padding:10px 20px;vertical-align:middle">
      <strong>University of Science and Technology of China</strong>, Hefei, China
      <br>
      PhD Student, Sept. 2021 - Present
    </td>
  </tr>

  <tr></tr>
    <td style="padding:10px 20px;vertical-align:middle">
      <strong>Sun Yat-sen University</strong>, Guangzhou, China
      <br>
      Undergraduate Student, Sept. 2017 - Jun. 2021
    </td>
  </tr>
  